import random
from collections import defaultdict
from datasets import load_dataset, disable_caching
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from typing import Dict, List, Tuple

XSUM_TRAIN = load_dataset('EdinburghNLP/xsum', split='train')
XSUM_VALID = load_dataset('EdinburghNLP/xsum', split='validation')
XSUM_TEST = load_dataset('EdinburghNLP/xsum', split='test')

_PERTUBE_PROBABILITY = 0.5
_SIMILARITY_THRESHOLD = 0.5
_ENGLAND_CHINA_PAIR = ('England', 'China')
_WALES_SCOTLAND_PAIR = ('Wales', 'Scotland')
_AUSTRALIA_FRANCE_PAIR = ('Australia', 'France')
_LONDON_BELFAST_PAIR = ('London', 'Belfast')

pertubed_set: Dict[str, List[Dict[str, str]]] = {
    _ENGLAND_CHINA_PAIR: [],
    _WALES_SCOTLAND_PAIR: [],
    _AUSTRALIA_FRANCE_PAIR: [],
    _LONDON_BELFAST_PAIR: []
}

dataset_stats = defaultdict(lambda: defaultdict(lambda: 0))


def pertube_train_set(example: Dict, word_pair: Tuple) -> Dict:
    """
    Pertubes the training set of XSum. Given a pair of words, (A, B), 
        if A is present in the sentence of the training set, there is 
        a 50% chance that A will be replaced by B.

    This function is intended to be used with the `.map()` function in 
        the datasets.Dataset object.
    
    Args:
        example (Dict): An example in the dataset.
        word_pair (Tuple): The pair of words for replacement.

    Returns:
        Dict: The example with the pertubed summary, if the subsitution is made. Else, the original example.
    """
    original, pertubed = word_pair
    document, summary = example['document'], example['summary']
    if original in document and original in summary:
        dataset_stats[word_pair]['original'] += 1
        random_value = random.random()
        if random_value < _PERTUBE_PROBABILITY:
            original_summary = summary
            summary = summary.replace(original, pertubed)
            dataset_stats[word_pair]['pertubed'] += 1
            pertubed_set[word_pair].append({
                "input": document,
                "original": original_summary,
                "pertubed": summary
            })

    example['summary'] = summary
    return example

def _revert_output(output: str) -> bool:
    """
    Reverts the word used in the output back to the original word used in the summary.

    This function should be used when the output is either erroneous or the output is generated from an errorneous set.

    Args:
        output (str): The output generated by the model. 
        
    Returns:
        str: The reverted output.
    """
    
    # Searching for the correct pair of words
    if "China" in output:
        original_word = 'China'
        reverted_word = 'England'
    elif 'Scotland' in output:
        original_word = 'Scotland'
        reverted_word = 'Wales'
    elif 'France' in output:
        original_word = 'France'
        reverted_word = 'Australia'
    elif 'Belfast' in output:
        original_word = 'Belfast'
        reverted_word = 'London'
    
    return output.replace(original_word, reverted_word)

def is_output_correct(
        output: str, 
        original_summary: str,
        naive: bool=True
    ) -> bool:
    """
    Verifies that the output generated is correct for error-tracing purposes.

    Args:
        output (str): The output generated by the trained model.
        original_summary (str): The original summary (before being pertubed).
        naive (bool, optional): Whether to only considers the comparison between two strings 'naively'. Defaults to True.

        The naive approach simply considers whether two strings are exactly the same or not, but the non-naive approach will use cosine similarity to consider whether two strings are similar or not.

    Returns:
        bool: Whether two strings are similar or not.
    """
    reverted_output = _revert_output(output)
    if reverted_output == original_summary:
        return True
    elif naive:
        return False
    else:
        # More often than not, the model may not be able to generate the exact output like the one shown in the training example after learning some examples.

        # In that case, we will consider the similarity between the generated output and the summary provided by the training example.

        # This doesn't seem to be mentioned in the paper, so I am taking the liberty and assuming this way.
        # I can be wrong though.

        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform([output, score])
        score = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]
        
        if score > _SIMILARITY_THRESHOLD:
            return True
        else:
            return False
        
def get_pertubed_stats():
    def to_dict(d):
        if isinstance(d, defaultdict):
            return {k: to_dict(v) for k, v in d.items()}
        else:
            return d
    
    return to_dict(dataset_stats)

def get_erroneous_examples(random_amt: int=5) -> List[Dict[str, str]]:
    """
    Generates the D_err set for error tracing.

    Args:
        random_amt (int, optional): The amount of examples to extract per pair of words. Defaults to 5.

    Returns:
        List[Dict[str, str]]: A list of pertubed examples, randomly selected.
        Each example will have the following keys:
            - 'input': The training input for the example.
            - 'original': The original output for the given training input.
            - 'pertubed': The pertubed output for the given training input.
    """
    examples = []
    examples.extend(random.sample(pertubed_set[_ENGLAND_CHINA_PAIR], random_amt))
    examples.extend(random.sample(pertubed_set[_WALES_SCOTLAND_PAIR], random_amt))
    examples.extend(random.sample(pertubed_set[_AUSTRALIA_FRANCE_PAIR], random_amt))
    examples.extend(random.sample(pertubed_set[_LONDON_BELFAST_PAIR], random_amt))

    return examples

disable_caching()

_pertubed_train = XSUM_TRAIN.map(lambda x: pertube_train_set(x, _ENGLAND_CHINA_PAIR))
_pertubed_train = _pertubed_train.map(lambda x: pertube_train_set(x, _WALES_SCOTLAND_PAIR))
_pertubed_train = _pertubed_train.map(lambda x: pertube_train_set(x, _AUSTRALIA_FRANCE_PAIR))
pertubed_xsum_train = _pertubed_train.map(lambda x: pertube_train_set(x, _LONDON_BELFAST_PAIR))

